{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitbaseconda6a1770027b924823bf65f396110ad90f",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "bc4fbd6e1bea1fda9411923f753f92d3282c070fdbd959891bf5a78b35520a26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "from Model import *\n",
    "from dataset import Dataset\n",
    "import numpy as np\n",
    "from Optimizer import Optimizer\n",
    "from Loss import *\n",
    "from Layer import backproplogger\n",
    "backproplogger.setLevel(logging.WARNING)\n",
    "\n",
    "# logger = logging.getLogger('simple_example')\n",
    "# logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_errors(yhati, yi):\n",
    "    errcount = 0\n",
    "    for yhat, y in zip(yhati, yi):\n",
    "        if (np.argmax(yhat) != np.argmax(y)):\n",
    "            errcount += 1\n",
    "    return(errcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dataset(\"../datasets/dataset_train.csv\")\n",
    "m = Model(sizes = [8, 20, 20, 10, 4], activations = [\"sigmoid\", \"sigmoid\", \"sigmoid\", \"softmax\"], optimizer=Optimizer(learning_rate=0.1, Loss=CrossEntropyLoss()))\n",
    "test = np.genfromtxt(\"../data/trainout.csv\", delimiter=\",\")\n",
    "l = m.layers[-1]\n",
    "d.x = d.x\n",
    "d.y = d.y\n",
    "d.y[0][2] = 0\n",
    "d.y[0][0] = 1\n",
    "mask = []\n",
    "for i in range(1600):\n",
    "    mask.append(i)\n",
    "ex = d.x[mask]\n",
    "ey = d.y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "l.z:\n[[-0.01 -6.01  4.49  1.25]\n [-2.89 -0.03 -0.19  3.79]\n [ 0.09 -6.41  4.87  1.12]\n ...\n [ 4.09 -1.09 -1.29 -2.04]\n [ 0.08  5.05 -5.17  0.91]\n [ 0.45  4.98 -5.21  0.57]]\n\nl.a:\n[[ 0.01  0.00  0.95  0.04]\n [ 0.00  0.02  0.02  0.96]\n [ 0.01  0.00  0.97  0.02]\n ...\n [ 0.99  0.01  0.00  0.00]\n [ 0.01  0.98  0.00  0.02]\n [ 0.01  0.98  0.00  0.01]]\n\ny:\n[[1 0 0 0]\n [0 0 0 1]\n [0 0 1 0]\n ...\n [1 0 0 0]\n [0 1 0 0]\n [0 1 0 0]]\n\nx:\n[[ 1.00 -1.03  0.88 ...  1.04  0.36  0.21]\n [ 1.00 -1.16 -1.39 ... -0.54 -1.21  0.65]\n [ 1.00 -0.80  1.26 ...  1.86  1.02  1.32]\n ...\n [ 1.00  0.97 -0.85 ... -0.32  1.33 -1.75]\n [ 1.00  0.79  0.44 ... -1.25 -1.06  1.32]\n [ 1.00  1.25  0.82 ... -0.01 -1.48  0.10]]\n\nl.w:\n[[ 0.05 -0.30 -0.23  0.28]\n [-1.09 -1.08  1.38  0.70]\n [ 2.02 -2.30  0.90 -0.63]\n [ 1.77  2.82 -2.51 -1.88]\n [-0.50 -1.61  2.48 -0.39]\n [ 1.55 -1.47  0.21 -0.90]\n [-0.81  0.27 -0.54  1.25]\n [-0.61 -1.10  1.15  0.59]\n [-1.49  0.20  0.35  1.50]\n [ 0.30  1.53 -1.16 -0.39]\n [-0.71  1.97 -2.01  1.09]]\n\n\n30\n"
     ]
    }
   ],
   "source": [
    "from Loss import losslogger\n",
    "from Layer import backproplogger\n",
    "backproplogger.setLevel(logging.WARN)\n",
    "losslogger.setLevel(logging.WARN)\n",
    "x = ex\n",
    "y = ey\n",
    "for i in range(1000):\n",
    "    # logging.debug(f\"\\n\\n{i}\\n\\n\")\n",
    "    m.fit(x, y)\n",
    "    # print(\"\\n\")\n",
    "print(f\"l.z:\\n{l.z}\\n\\nl.a:\\n{l.a}\\n\\ny:\\n{y}\\n\\nx:\\n{x}\\n\\nl.w:\\n{l.w}\\n\\n\")\n",
    "print(count_errors(l.a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.04, -0.03,  0.07,  0.15,  0.16, -0.05,  0.00, -0.03, -0.02,\n",
       "         0.01, -0.09,  0.14,  0.01, -0.11,  0.12, -0.05,  0.04, -0.03,\n",
       "         0.12, -0.03],\n",
       "       [-0.05, -0.03,  0.08,  0.16,  0.16, -0.05, -0.00, -0.03, -0.02,\n",
       "         0.02, -0.08,  0.15,  0.01, -0.11,  0.12, -0.06,  0.04, -0.03,\n",
       "         0.11, -0.03],\n",
       "       [ 0.15, -0.24,  0.14, -0.03,  0.06, -0.30, -0.06, -0.03,  0.13,\n",
       "        -0.19, -0.45,  0.45,  0.13, -0.32,  0.22, -0.07,  0.20, -0.10,\n",
       "         0.43, -0.41],\n",
       "       [-0.18,  0.13,  0.03,  0.12,  0.10,  0.07,  0.16, -0.12,  0.10,\n",
       "        -0.15, -0.24,  0.20, -0.08, -0.27, -0.10,  0.01, -0.15, -0.17,\n",
       "         0.23, -0.16],\n",
       "       [-0.15,  0.27, -0.11,  0.02, -0.08,  0.30,  0.05,  0.04, -0.15,\n",
       "         0.21,  0.50, -0.49, -0.13,  0.37, -0.23,  0.04, -0.21,  0.12,\n",
       "        -0.48,  0.46],\n",
       "       [ 0.27, -0.12,  0.72, -0.47, -0.37, -0.25, -0.42,  0.50, -0.14,\n",
       "        -0.01, -0.44,  0.40,  0.10, -0.28, -0.00, -0.59, -0.01,  0.07,\n",
       "         0.38, -0.33],\n",
       "       [ 0.02,  0.16,  0.34, -0.27, -0.28,  0.14, -0.19,  0.34, -0.22,\n",
       "         0.17,  0.20, -0.17, -0.07,  0.18, -0.20, -0.29, -0.20,  0.15,\n",
       "        -0.16,  0.26],\n",
       "       [ 0.41, -0.06,  0.89, -0.77, -0.70, -0.10, -0.69,  0.87, -0.47,\n",
       "         0.34,  0.06, -0.05,  0.08,  0.26, -0.15, -0.80, -0.09,  0.39,\n",
       "        -0.08,  0.12],\n",
       "       [-0.41,  0.30, -0.46,  0.39,  0.34,  0.29,  0.44, -0.46,  0.16,\n",
       "        -0.07,  0.24, -0.27, -0.18,  0.01, -0.17,  0.40, -0.18, -0.16,\n",
       "        -0.27,  0.18]])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "m.layers[0].w - qw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 0.01  0.00  0.95  0.04] [1 0 0 0]\n[ 0.00  0.03  0.01  0.95] [0 0 1 0]\n[ 0.01  0.98  0.00  0.01] [0 0 1 0]\n[ 0.01  0.00  0.96  0.03] [0 0 0 1]\n[ 0.01  0.98  0.00  0.01] [0 0 1 0]\n[ 0.01  0.98  0.00  0.02] [0 0 1 0]\n[ 0.01  0.00  0.97  0.02] [0 0 0 1]\n[ 0.99  0.01  0.00  0.00] [0 0 1 0]\n[ 0.01  0.00  0.95  0.03] [1 0 0 0]\n[ 0.01  0.00  0.92  0.07] [1 0 0 0]\n[ 0.00  0.01  0.04  0.94] [0 0 1 0]\n[ 0.02  0.00  0.97  0.02] [1 0 0 0]\n[ 0.01  0.00  0.97  0.02] [0 1 0 0]\n[ 0.07  0.92  0.00  0.01] [1 0 0 0]\n[ 0.02  0.97  0.00  0.01] [0 0 0 1]\n[ 0.02  0.97  0.00  0.01] [1 0 0 0]\n[ 0.00  0.02  0.02  0.96] [0 1 0 0]\n[ 0.01  0.97  0.00  0.02] [0 0 0 1]\n[ 0.99  0.01  0.00  0.00] [0 0 1 0]\n[ 0.99  0.01  0.00  0.00] [0 1 0 0]\n[ 0.99  0.01  0.00  0.00] [0 1 0 0]\n[ 0.01  0.00  0.98  0.01] [0 0 0 1]\n[ 0.01  0.00  0.96  0.03] [0 0 0 1]\n[ 0.02  0.97  0.00  0.01] [0 0 1 0]\n[ 0.01  0.00  0.98  0.01] [0 0 0 1]\n[ 0.01  0.96  0.00  0.03] [1 0 0 0]\n[ 0.01  0.00  0.97  0.02] [1 0 0 0]\n[ 0.01  0.00  0.97  0.02] [1 0 0 0]\n[ 0.01  0.98  0.00  0.02] [1 0 0 0]\n[ 0.01  0.96  0.00  0.03] [0 0 0 1]\n30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "qw = [[-0.25,  0.17, -0.14,  0.04, -0.34,  0.29,  0.28, -0.33,  0.32,\n",
    "        -0.26, -0.17,  0.09,  0.31,  0.22, -0.35,  0.01,  0.03, -0.01,\n",
    "         0.20, -0.22],\n",
    "       [ 0.19, -0.34, -0.24, -0.26, -0.13,  0.12, -0.02,  0.22, -0.15,\n",
    "         0.16,  0.12, -0.11, -0.12,  0.31,  0.09,  0.32,  0.18,  0.23,\n",
    "        -0.05, -0.01],\n",
    "       [-0.03,  0.27, -0.27,  0.37, -0.03, -0.11, -0.02, -0.22,  0.02,\n",
    "        -0.40, -0.46,  0.31,  0.09, -0.44, -0.04, -0.18, -0.27, -0.02,\n",
    "         0.09, -0.47],\n",
    "       [-0.03,  0.29, -0.31, -0.14,  0.12,  0.07,  0.10, -0.07,  0.31,\n",
    "         0.09,  0.18, -0.18, -0.07,  0.32, -0.29,  0.28, -0.38,  0.06,\n",
    "        -0.40,  0.21],\n",
    "       [ 0.06,  0.33,  0.35, -0.08,  0.10,  0.36,  0.08,  0.01, -0.18,\n",
    "        -0.08,  0.49, -0.22,  0.32,  0.35,  0.03, -0.37,  0.24,  0.33,\n",
    "        -0.50,  0.25],\n",
    "       [-0.09,  0.07,  0.08,  0.03, -0.14, -0.29,  0.22,  0.27, -0.20,\n",
    "        -0.21, -0.08, -0.11,  0.25, -0.19, -0.08,  0.07, -0.02, -0.12,\n",
    "         0.18, -0.22],\n",
    "       [-0.26,  0.02,  0.25, -0.25, -0.31,  0.38, -0.26,  0.20, -0.18,\n",
    "         0.16,  0.30, -0.50, -0.11,  0.01, -0.36, -0.40, -0.42,  0.11,\n",
    "        -0.46,  0.06],\n",
    "       [ 0.36,  0.27,  0.38, -0.36, -0.19, -0.18, -0.10,  0.18, -0.11,\n",
    "         0.22,  0.18,  0.03, -0.22,  0.26,  0.05, -0.19, -0.10,  0.39,\n",
    "        -0.21,  0.01],\n",
    "       [-0.01,  0.17, -0.39,  0.11,  0.23,  0.39,  0.25, -0.30, -0.16,\n",
    "        -0.03,  0.21, -0.41, -0.13, -0.09,  0.25,  0.33,  0.11,  0.00,\n",
    "        -0.31,  0.39]]\n",
    "c = 0\n",
    "i = 0\n",
    "for x, y in zip(m.feed_forward(d.x), d.y):\n",
    "    if (np.argmax(x) != np.argmax(y)):\n",
    "        print(x, y)\n",
    "        c += 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 1.00,  0.00,  0.00,  0.00],\n",
       "       [ 0.00,  0.00,  0.00,  1.00],\n",
       "       [ 0.00,  0.00,  1.00,  0.00],\n",
       "       ...,\n",
       "       [ 1.00,  0.00,  0.00,  0.00],\n",
       "       [ 0.00,  1.00,  0.00,  0.00],\n",
       "       [ 0.00,  1.00,  0.00,  0.00]])"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "m.feed_forward(d.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n3\n2\n0\n0\n3\n0\n1\n0\n1\n1\n1\n1\n3\n2\n2\n0\n0\n2\n1\n0\n0\n2\n1\n0\n1\n3\n1\n0\n1\n1\n0\n2\n1\n0\n0\n1\n1\n3\n1\n0\n1\n0\n1\n1\n0\n0\n1\n3\n0\n0\n3\n3\n0\n0\n3\n1\n3\n1\n0\n3\n2\n1\n1\n1\n1\n1\n0\n0\n1\n2\n3\n3\n1\n2\n1\n0\n0\n1\n1\n1\n3\n3\n0\n1\n1\n1\n1\n1\n0\n0\n0\n0\n1\n1\n1\n0\n2\n3\n1\n0\n1\n0\n1\n0\n0\n3\n0\n1\n3\n3\n2\n0\n0\n0\n0\n0\n3\n1\n1\n0\n3\n0\n1\n1\n3\n1\n3\n0\n1\n3\n0\n3\n1\n2\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n3\n0\n0\n0\n0\n0\n0\n3\n3\n1\n0\n1\n2\n0\n1\n1\n1\n3\n0\n1\n1\n1\n3\n0\n2\n0\n0\n2\n1\n0\n0\n1\n0\n0\n3\n1\n0\n3\n3\n1\n1\n3\n1\n0\n3\n1\n0\n1\n3\n1\n1\n0\n0\n3\n1\n0\n0\n0\n0\n1\n0\n1\n1\n0\n1\n0\n3\n3\n2\n1\n0\n2\n0\n0\n1\n1\n0\n3\n0\n2\n1\n0\n1\n1\n1\n0\n1\n2\n1\n1\n3\n0\n1\n1\n1\n0\n1\n1\n1\n1\n0\n1\n0\n0\n0\n3\n3\n0\n1\n2\n2\n3\n0\n2\n3\n1\n0\n0\n3\n3\n3\n0\n1\n0\n0\n0\n1\n1\n0\n0\n2\n1\n1\n0\n1\n2\n1\n1\n0\n0\n0\n1\n1\n0\n0\n1\n1\n1\n0\n0\n1\n0\n0\n1\n0\n1\n3\n1\n1\n0\n3\n0\n1\n3\n2\n1\n3\n1\n0\n0\n0\n3\n3\n1\n1\n3\n3\n0\n1\n0\n0\n0\n0\n0\n3\n1\n1\n1\n0\n0\n3\n0\n0\n1\n3\n0\n0\n1\n3\n0\n3\n0\n1\n1\n0\n0\n0\n0\n1\n0\n3\n1\n0\n0\n1\n1\n0\n0\n3\n2\n3\n1\n0\n1\n0\n1\n1\n1\n1\n2\n0\n1\n3\n0\n1\n0\n3\n1\n0\n1\n0\n1\n0\n1\n1\n3\n1\n1\n0\n2\n0\n3\n0\n0\n0\n1\n3\n1\n2\n1\n0\n0\n3\n0\n3\n0\n0\n0\n1\n0\n3\n1\n3\n1\n1\n3\n1\n1\n3\n0\n1\n1\n1\n0\n0\n0\n1\n0\n1\n1\n0\n0\n3\n0\n1\n2\n2\n0\n0\n1\n1\n0\n0\n0\n1\n1\n3\n1\n3\n3\n2\n0\n1\n0\n1\n0\n0\n0\n1\n0\n0\n1\n3\n0\n0\n0\n1\n1\n1\n0\n1\n1\n0\n2\n1\n1\n0\n3\n1\n1\n0\n0\n1\n1\n0\n1\n3\n3\n1\n1\n3\n0\n0\n2\n0\n1\n1\n1\n0\n0\n3\n0\n1\n1\n0\n1\n0\n1\n1\n2\n0\n0\n0\n0\n1\n0\n3\n0\n3\n0\n0\n0\n0\n0\n2\n1\n0\n2\n0\n1\n0\n2\n3\n2\n3\n0\n2\n0\n0\n2\n0\n3\n1\n0\n1\n3\n0\n1\n1\n1\n0\n1\n1\n2\n3\n0\n0\n0\n1\n1\n1\n0\n0\n1\n1\n1\n0\n0\n1\n0\n1\n1\n0\n3\n0\n0\n1\n0\n0\n1\n0\n3\n3\n0\n0\n3\n3\n1\n0\n0\n1\n0\n0\n3\n0\n0\n3\n0\n0\n0\n0\n0\n3\n1\n0\n0\n3\n1\n0\n0\n0\n0\n0\n1\n3\n3\n0\n1\n0\n0\n1\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0\n3\n2\n1\n1\n3\n3\n0\n1\n3\n1\n1\n1\n3\n3\n3\n1\n3\n0\n1\n0\n0\n0\n1\n3\n1\n3\n0\n0\n1\n3\n1\n1\n1\n0\n0\n3\n0\n1\n2\n1\n0\n0\n1\n3\n3\n0\n1\n2\n3\n3\n1\n2\n0\n0\n2\n0\n3\n0\n0\n0\n0\n2\n0\n1\n1\n2\n0\n0\n1\n1\n0\n0\n0\n0\n0\n2\n0\n3\n3\n1\n1\n2\n0\n0\n0\n3\n1\n3\n1\n3\n0\n0\n2\n0\n0\n3\n1\n3\n0\n0\n3\n1\n0\n1\n1\n1\n0\n0\n0\n0\n3\n1\n0\n1\n2\n1\n1\n0\n0\n1\n2\n1\n0\n1\n3\n0\n1\n1\n1\n0\n3\n1\n3\n0\n0\n0\n1\n0\n0\n3\n1\n3\n0\n3\n2\n0\n3\n1\n1\n0\n3\n0\n0\n2\n1\n0\n1\n0\n0\n0\n2\n0\n0\n3\n1\n0\n2\n2\n0\n3\n1\n0\n0\n3\n1\n3\n1\n0\n0\n3\n0\n0\n0\n1\n1\n3\n2\n1\n0\n1\n2\n0\n3\n0\n0\n0\n1\n1\n0\n1\n0\n0\n0\n1\n0\n3\n0\n1\n0\n1\n0\n1\n1\n0\n3\n0\n1\n3\n3\n3\n3\n0\n0\n0\n0\n0\n1\n1\n0\n3\n1\n0\n0\n3\n3\n1\n3\n1\n1\n1\n0\n0\n0\n0\n3\n1\n3\n0\n0\n3\n3\n3\n3\n3\n1\n1\n3\n0\n3\n0\n3\n1\n1\n0\n0\n1\n1\n3\n3\n1\n3\n0\n1\n0\n1\n0\n0\n0\n1\n1\n0\n0\n1\n0\n0\n3\n0\n1\n0\n0\n0\n1\n0\n3\n3\n1\n0\n0\n3\n1\n1\n0\n0\n0\n3\n0\n2\n1\n0\n1\n1\n0\n1\n1\n0\n3\n1\n1\n3\n0\n1\n1\n1\n1\n0\n0\n2\n1\n1\n1\n1\n0\n1\n1\n3\n0\n0\n0\n0\n3\n1\n0\n0\n0\n0\n2\n2\n1\n3\n2\n0\n0\n2\n3\n1\n2\n1\n0\n3\n0\n1\n1\n1\n0\n0\n0\n0\n3\n0\n1\n2\n0\n0\n1\n3\n1\n0\n0\n0\n1\n1\n0\n0\n2\n3\n2\n0\n0\n1\n0\n1\n0\n1\n3\n1\n0\n0\n3\n0\n0\n3\n1\n0\n2\n3\n0\n2\n2\n0\n1\n0\n2\n0\n3\n0\n1\n3\n0\n0\n0\n2\n0\n0\n0\n0\n1\n3\n2\n1\n1\n0\n1\n0\n1\n3\n1\n1\n3\n3\n0\n1\n1\n3\n2\n3\n3\n1\n0\n0\n1\n0\n0\n1\n0\n1\n1\n1\n1\n1\n2\n0\n0\n1\n3\n0\n2\n0\n0\n1\n1\n1\n2\n1\n1\n3\n1\n0\n1\n1\n1\n1\n3\n0\n0\n2\n3\n3\n0\n0\n3\n0\n3\n1\n3\n1\n1\n3\n0\n0\n0\n2\n2\n3\n1\n0\n3\n1\n0\n3\n0\n0\n1\n3\n3\n3\n2\n0\n0\n0\n0\n1\n3\n0\n1\n3\n0\n1\n0\n1\n0\n0\n0\n1\n0\n1\n1\n1\n0\n0\n1\n0\n1\n3\n3\n2\n1\n3\n1\n2\n0\n0\n1\n0\n3\n0\n0\n0\n0\n0\n0\n0\n1\n0\n3\n1\n1\n0\n0\n1\n0\n3\n3\n0\n1\n1\n3\n1\n1\n0\n3\n0\n0\n2\n3\n1\n3\n0\n1\n0\n0\n3\n3\n3\n0\n2\n0\n1\n1\n1\n0\n0\n1\n3\n1\n3\n1\n0\n1\n1\n0\n2\n2\n1\n1\n1\n1\n1\n3\n2\n0\n0\n0\n0\n1\n1\n1\n3\n3\n0\n3\n3\n1\n0\n1\n0\n3\n0\n1\n1\n1\n0\n1\n1\n2\n1\n2\n1\n3\n0\n1\n1\n0\n1\n0\n1\n3\n3\n0\n3\n0\n1\n2\n0\n3\n0\n1\n1\n2\n3\n1\n0\n0\n0\n0\n1\n1\n0\n1\n1\n1\n3\n3\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n3\n2\n1\n0\n1\n3\n0\n1\n3\n0\n1\n3\n1\n0\n3\n3\n0\n0\n0\n1\n2\n1\n1\n0\n0\n1\n3\n0\n0\n2\n1\n1\n3\n2\n1\n0\n0\n2\n1\n3\n1\n0\n0\n0\n3\n1\n0\n1\n2\n0\n1\n1\n0\n0\n0\n0\n1\n2\n3\n0\n1\n0\n0\n1\n0\n0\n3\n3\n0\n0\n0\n0\n0\n1\n1\n0\n1\n3\n0\n0\n1\n3\n0\n0\n2\n1\n0\n1\n0\n3\n1\n1\n1\n0\n2\n3\n0\n1\n0\n0\n1\n3\n0\n1\n1\n0\n3\n1\n0\n2\n0\n1\n3\n0\n3\n0\n0\n3\n1\n2\n1\n1\n0\n0\n1\n2\n0\n0\n1\n1\n0\n1\n1\n1\n0\n3\n1\n3\n1\n0\n1\n3\n2\n2\n3\n3\n0\n1\n1\n0\n1\n1\n3\n1\n0\n0\n1\n0\n1\n0\n0\n1\n3\n1\n0\n0\n1\n0\n0\n1\n0\n1\n3\n3\n1\n1\n0\n2\n0\n0\n0\n0\n1\n2\n0\n0\n0\n2\n0\n1\n1\n1\n1\n1\n0\n1\n0\n3\n0\n0\n0\n1\n1\n0\n3\n1\n1\n0\n1\n1\n3\n1\n0\n3\n0\n3\n2\n0\n1\n2\n0\n2\n1\n2\n0\n2\n0\n2\n1\n0\n0\n0\n0\n0\n0\n1\n3\n1\n0\n0\n0\n2\n0\n0\n3\n1\n3\n0\n3\n3\n1\n3\n1\n3\n1\n2\n3\n0\n0\n1\n0\n1\n0\n1\n1\n"
     ]
    }
   ],
   "source": [
    "for x in m.feed_forward(d.x):\n",
    "    print(np.argmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "DEBUG:root:y:\n",
      "[[0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " ...\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]]\n",
      "DEBUG:root:yhat:\n",
      "[[ 0.09  0.57  0.22  0.12]\n",
      " [ 0.02  0.35  0.50  0.13]\n",
      " [ 0.06  0.67  0.14  0.14]\n",
      " ...\n",
      " [ 0.27  0.21  0.38  0.15]\n",
      " [ 0.29  0.31  0.15  0.25]\n",
      " [ 0.39  0.23  0.10  0.29]]\n",
      "DEBUG:root:-y / yhat:\n",
      "[[ 0.00  0.00 -4.59  0.00]\n",
      " [ 0.00  0.00  0.00 -7.53]\n",
      " [ 0.00  0.00 -7.34  0.00]\n",
      " ...\n",
      " [-3.74  0.00  0.00  0.00]\n",
      " [ 0.00 -3.19  0.00  0.00]\n",
      " [ 0.00 -4.33  0.00  0.00]]\n"
     ]
    }
   ],
   "source": [
    "m.feed_forward(d.x)\n",
    "djda = m.Optimizer.Loss.loss_derivative(m.layers[-1].a, d.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = m.layers[-1]\n",
    "dadz = l.activation_derivative(l.z, l.a)\n",
    "# print(f\"{dadz.shape = }, {djda.shape = }\")\n",
    "# logging.debug(f\"djda:\\n {djda}\\n\")\n",
    "djdz = np.einsum( 'ik,ikj->ij', djda, dadz)\n",
    "djdz2 = l.a - d.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0.00,  0.00,  0.00,  0.00],\n",
       "       [ 0.00,  0.00,  0.00,  0.00],\n",
       "       [ 0.00,  0.00,  0.00,  0.00],\n",
       "       ...,\n",
       "       [ 0.00,  0.00,  0.00,  0.00],\n",
       "       [ 0.00, -0.00,  0.00,  0.00],\n",
       "       [ 0.00,  0.00,  0.00,  0.00]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.09  0.57  0.22  0.12]\n [ 0.02  0.35  0.50  0.13]\n [ 0.06  0.67  0.14  0.14]\n ...\n [ 0.27  0.21  0.38  0.15]\n [ 0.29  0.31  0.15  0.25]\n [ 0.39  0.23  0.10  0.29]]\n[[0 0 1 0]\n [0 0 0 1]\n [0 0 1 0]\n ...\n [1 0 0 0]\n [0 1 0 0]\n [0 1 0 0]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-4.545454545454546"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "print(l.a)\n",
    "print(d.y)\n",
    "-1 / 0.22"
   ]
  }
 ]
}